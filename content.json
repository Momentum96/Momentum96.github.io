{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Create, Operate, Convert Pytorch Tensor","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Pytorch? Python Library Helps Build Deep Learning ProjectsProvides a tensor, a core data structure (multi-dimensional array similar to the numpy array)Tensor accelerates mathematical operations (GPU available) It is mostly made of C++ and CUDA for performance reasons. Installation PyTorch Installation Link Copy Run this Command after setting it to your environment. Check GPU 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Create Tensor 12345678910111213141516# The values that existed in the memory allocated at that time appear as# initial values.x = torch.empty(5, 3)# Randomly initialized matrix (0 = x &lt; 1)x = torch.rand(5, 3) # dtype = long, Matrix filled with zerosx = torch.zeros(5, 3, dtype=torch.long) # Create tensor with listx = torch.tensor([5.5, 3]) # Create new Tensor based on existing Tensorx = x.new_ones(5, 3, dtype=torch.double) # Create new Tensor based on existing Tensorx = torch.randn_like(x, dtype=torch.float) # Obtain matrix size, return torch.Size supports tuple types, all tuple operationsx.size() Tensor Operation 1234567891011# Operation 1y = torch.rand(5, 3) print(x + y)# Operation 2print(torch.add(x, y)) result = torch.empty(5, 3)# Operation 3torch.add(x, y, out=result) print(result) Change Tensor shape and convert to numpy array 1234567891011121314151617# When changing size and shape of sensor, use torch.viewx = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# If only one value exists in the tensor, a numeric value can be obtained# using .item()print(x.item()) # Convert torch tensor to numpy arraya = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor Change 123456789101112131415import torchimport numpy as npx = torch.randn(1)# Run only in CUDA-enabled environments (GPU environments)if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # Create a direct tensor on a GPU y = torch.ones_like(x, device=device) # Change CPU Tensor to GPU Tensor x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/en/Pytorch-Tensor/"},{"title":"Pytorch 텐서 생성, 연산, 변환","text":"This article is also available in English. Pytorch란? 딥러닝 프로젝트 빌드를 위해 도움을 주는 파이썬 라이브러리코어 데이터 구조인 텐서를 제공 (numpy 배열과 유사한 다차원 배열)텐서는 수학적 연산을 가속화 (GPU 사용 가능) 성능적인 이유로 대부분 C++과 CUDA로 만들어져있음. 설치방법 PyTorch 설치 링크 본인의 환경에 맞게 설정 후 Run this Command 명령어 복사 GPU 확인 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Tensor 생성 123456789101112131415# 그 시점에 할당된 메모리에 존재하던 값들이 초기값으로 나타남x = torch.empty(5, 3)# 무작위로 초기화된 행렬 (0 &lt;= x &lt; 1)x = torch.rand(5, 3) # dtype = long, 0으로 채워진 행렬x = torch.zeros(5, 3, dtype=torch.long) # list를 사용하여 직접 tensor 생성x = torch.tensor([5.5, 3]) # 기존 tensor를 바탕으로 새로운 tensor 생성x = x.new_ones(5, 3, dtype=torch.double) # 기존 tensor를 바탕으로 새로운 tensor 생성x = torch.randn_like(x, dtype=torch.float) # 행렬 크기 구하기, 반환인 torch.Size는 튜플 타입, 모든 튜플 연산 지원x.size() Tensor 연산 1234567891011# 연산 문법 1y = torch.rand(5, 3) print(x + y)# 연산 문법 2print(torch.add(x, y)) result = torch.empty(5, 3)# 연산 문법 3torch.add(x, y, out=result) print(result) Tensor shape 변경 및 numpy 배열로 변환 12345678910111213141516# tensor의 size, shape 변경 시에는 torch.view 사용x = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# tensor에 하나의 값만 존재한다면 .item()을 사용하여 숫자 값 얻을 수 있음print(x.item()) # torch tensor를 numpy 배열로 변환a = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor 변환 123456789101112131415import torchimport numpy as npx = torch.randn(1)# CUDA 사용 가능 환경(GPU 환경)에서만 실행if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # GPU 상 직접 tensor 생성 y = torch.ones_like(x, device=device) # CPU 텐서를 GPU 텐서로 변경 x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/kr/Pytorch-Tensor/"},{"title":"Pytorch Autograd","text":"This article is also available in English. Autogard (자동 미분) autogard 패키지는 Tensor의 모든 연산에 대한 자동 미분 제공 torch.Tensor 클래스에서 .requires_gard 속성을 True로 설정하면 해당 tensor에서 이뤄진 모든 연산들을 추적(track) 계산이 완료된 후 .backward()**를 호출하여 **변화도(gradient)를 자동으로 계산할 수 있음 Tensor가 기록 추적하는 것을 중단시키려면 .detach()**를 호출하여 연산 기록으로부터 분리할 수 있음 기록 추적 및 메모리 사용을 방지하기 위해 코드 블럭을 **with torch.no_gard(): 로 감쌀 수 있음 변화도(gradient)는 필요 없지만 requires_guard=True로 설정되어 학습 가능한 매개변수를 갖는 모델 평가 시 유용 Function 클래스 Tensor와 Function은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프를 생성 각 Tensor는 .gard_fn 속성을 갖는데, 이는 Tensor를 생성한 Function을 참고함 (사용자 생성 Tensor의 grad_fn은 None) 도함수 계산은 Tensor의 .backward() 호출 Tensor가 스칼리인 경우 backward에 인자 필요 없으나 여러 요소를 가질 때는 tensor의 모양을 gradient의 인자로 지정할 필요가 있음","link":"/2021/01/25/kr/Pytorch-Autograd/"},{"title":"Pytorch Autograd","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Autogard autogard package provides automatic differentiation for all operations of the Tensor. Setting the .requires_gard property to True in the torch.Tensor class tracks all operations performed on that sensor. After the calculation is complete, you can call .backward() to automatically calculate the gradient To stop Tensor from tracking records, you can call .detach() to separate from the computational history Code blocks can be wrapped with torch.no_gard(): to avoid historical tracking and memory usage Useful for evaluating models with learnable parameters, set to require_guard=True, although gradients are not required. Function Class Tensor and Function are interconnected and encode all computational processes to create non-circulating graphs Each Tensor has a .gard_fn property, which refers to the Function that generated the Tensor (The gradient_fn of the user-generated Tensor is None) The derivative calculation calls the .backward() of the Tensor Backward does not require factors if Tensor is Scally, but when it has multiple elements, it is necessary to shape the Tensor as a factor of gradient.","link":"/2021/01/25/en/Pytorch-Autograd/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"파이토치","slug":"파이토치","link":"/tags/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"}],"categories":[{"name":"Pytorch","slug":"Pytorch","link":"/categories/Pytorch/"}]}
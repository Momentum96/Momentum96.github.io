{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Machine Learning","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Machine Learning Algorithm techniques for learning patterns based on data and predicting resultsWhen developers create programs that take into account the nature of their data and work logic, useful for areas where difficulty and complexity of development are inevitably too high The field of machine learning Supervised Learning Classification Regression Recommanded System Visual, Voice Recognition Text analysis, NLP, etc. Unsupervised Learning Clustering Dimensionality Reduction Reinforcement learning, etc. The biggest drawback of machine learning Data-dependentMachine learning results can’t be good without good quality dataThe ability to build optimal machine learning algorithms and model parameters is important, but the ability to process, process, and extract data efficiently is also important.","link":"/2021/01/27/en/Machine-Learning/"},{"title":"Pytorch Neural Network","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Neural Network Can be created using the torch.nn packagenn uses autograd to define and differentiate modelsnn.Module contains forward(input) method that returns layer and output Typical Learning Process of Neural Networks Define a neural network with learnable parameters (or weights) Repeat Dataset Input Propagate inputs from neural networks Calculating loss Backprop the gradient to the neural network parameters. Updating the weight of a neural networks(New Weight = Existing Weight - Learning_rate * Gradient) Define Neural Network 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() &quot;&quot;&quot; 1 input image channel, 6 output channels, 3x3 square convolution matrix Convolutional Kernel Definitions troch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int) : Number of channel about input image. if rgb == 3 out_channels(int) : Number of channels created by convolution kernel_size(int or tuple) : convoling_kernel size. (filter) stride(int or tuple) : Stride size of convolution default is 1, stride is the step size of the kernel when traversing the image padding(int or tuple) : zero padding size Default is 0 so zero padding is not applied if not set by default self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) Affine operation : y=Wx+b self.fc1 = nn.Linear(16*6*6, 120) # 6*6 is image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) &quot;&quot;&quot; def forward(self, x): # Max pooling for (2, 2) size windows x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square number, specify only one number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # All dimensions except batch dimensions num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 2D convolution using a kernel size of 3, stride of 1 and padding If you define a forward function only, the backward function is automatically defined using autograd.You can use any Tensor operation in the forward function.The model’s learnable parameters are returned by net.parameters() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071params = list(net.parameters())print(len(params))print(params[0].size())# Enter any 32x32 valueinput = torch.randn(1, 1, 32, 32)out = net(input)print(out)# Set all parameters to zero degrees of change buffer,# backpropagating to random valuesnet.zero_grad()out.backward(torch.rand(1, 10))# loss function# Take output, target as a pair of inputs and calculate the estimate of# how far the output is from the correct answer.# nn.MSEloss calculates mean square error between output and target# as a simple loss functionoutput = net(input)target = torch.randn(10) # Example for comparison, random correct answertarget = target.view(1, -1) # Convert to same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target)print(loss)&quot;&quot;&quot;input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss&quot;&quot;&quot;# .grad Tensor with accumulated variabilityprint(loss.grad_fn)print(loss.grad_fn.next_functions[0][0])print(loss.grad_fn.next_functions[0][0].next_functions[0][0])# backprop# If you don't eliminate the traditional changes,# they accumulate in the existing ones.net.zero_grad()print('conv1.bias.grad before backward') # Initialized to 0print(net.conv1.bias.grad)loss.backward() # loss = criterion(output, target)print('conv1.bias.grad after backward')print(net.conv1.bias.grad)# Updating Weight# Stochastic gradient descent(SGD)learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate)# implemented various renewal rules such as SGD, Nesterov-SGD, Adam,and RMSProp# in a small package called torch.optimimport torch.optim as optim# Create Optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# Training process# Manually set the change chart buffer to zero using optimizer.zero_grad()optimizer.zero_grad()output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()","link":"/2021/01/26/en/Pytorch-Neural-Network/"},{"title":"Scikit-Learn-2","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Understanding the Estimator In scikit-learn, classification algorithms are referred to as Classifier and regressor for regression algorithms.The combination of Classifier and Regressor is called the Estimator class.Model learning provides fit() and predictions using learned models provide predict() methods. For classes that support evaluation functions such as cross_val_score(), and hyperparameter tuning such as GridSearchCV, the estimator is taken as a factor Most classes that implement dimensionality reduction, clustering, feature extraction, etc. that are unsupervised learners also apply fit(), transform().In unsupervised learning and feature extraction, fit() corresponds to a pre-structural task to transform data to fit the shape of the input data.When structured with fit(), real-world operations such as dimension conversion, clustering, feature extraction, etc. of subsequent input data are performed with transform()It also comes with fit_transform() that combines the two features. scikit-learn main package sklearn.datasets datasets.make_classificiationCreating a dataset for classificationRandomly generate data for noise effects such as high correlation and unnecessary properties datasets.make_bolbs()Randomize dataset creation for clusteringGenerate datasets for different clustering based on cluster-specified number The psychedelic dataset is in the form of a dictionary. data : Feature Dataset target : Label value at classification, numeric result value at regression target_names : Individual Label Name feature_names : Feature Name DESCR : Description of the dataset and each feature data, target is numpy arraytarget_names, feature_names is numpy array, or list type.DESCR is a string type","link":"/2021/01/27/en/Scikit-Learn-2/"},{"title":"Pytorch Autograd Practice","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchx = torch.ones(2, 2, requires_grad=True) # Record operationsprint(x)y = x + 2 # Performing operationsprint(y)print(y.grad_fn) # This is the result of the operation, so it has grad_fn.z = y*y*3 # z = (x+2)^2*3out = z.mean()print(z, out)&quot;&quot;&quot;.requires_gard_(...) changes the requirements_grad value of the existing Tensor toinplace. If no input value is specified, default is False&quot;&quot;&quot;a = torch.randn(2, 2) # default requires_gard = Falsea = ((a * 3) / (a - 1))print(a.requires_grad) # Falsea.requires_grad_(True) # Operate inplace because _ is attached to the behindprint(a.requires_grad) # Trueb = (a * a).sum() # Grad_fn because it is the result of an operation of Trueprint(b.grad_fn)print(b)# backprop&quot;&quot;&quot;1. backprop of scalar value = When the value is added to the differential result.x = a matrix containing two lines, two rows, 1y = x+2 == a matrix containing two-line, two-row, 3z = y*y*3 == a matrix containing two-line, two-row, 27 (3*(x+2)^2)out = Mean of z&quot;&quot;&quot;out.backward() # == out.backward(torch.tensor(1.))print(out)print(x.grad)# The differential for x in the final equation in which x was used.# (out = 3*(x+2)^2/4 -&gt; 3*(x+2)/2)# (x = 1) (x = 1 is set in out.backward())&quot;&quot;&quot;2. backprop of vector value = Jacobian Matrix (a matrix of all partial differential values for each dimension, given that there is a function from m to n.) Typical differential: deals with only one-variable functions Partial differential: In multivariate functions, only one variable is a variable and the other is a constant. The vector's backprop is any value multiplied by the partial differential matrix.&quot;&quot;&quot;x = torch.rand(3, requires_grad=True)y = x * 2 # 2xwhile y.data.norm() &lt; 1000: y = y * 2 # 2^n*xprint(y)v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v) # 2^n multiplied by 0.1, 1.0, 0.0001print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): # Stop Recording Tensor Operations print((x ** 2).requires_grad)print(x.requires_grad)y = x.detach() # Create a new Tensor with the same content but false require_gradprint(y.requires_grad)print(x.eq(y).all())","link":"/2021/01/26/en/Pytorch-Autograd-Practice/"},{"title":"Create, Operate, Convert Pytorch Tensor","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Pytorch? Python Library Helps Build Deep Learning ProjectsProvides a tensor, a core data structure (multi-dimensional array similar to the numpy array)Tensor accelerates mathematical operations (GPU available) It is mostly made of C++ and CUDA for performance reasons. Installation PyTorch Installation Link Copy Run this Command after setting it to your environment. Check GPU 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Create Tensor 12345678910111213141516# The values that existed in the memory allocated at that time appear as# initial values.x = torch.empty(5, 3)# Randomly initialized matrix (0 = x &lt; 1)x = torch.rand(5, 3) # dtype = long, Matrix filled with zerosx = torch.zeros(5, 3, dtype=torch.long) # Create tensor with listx = torch.tensor([5.5, 3]) # Create new Tensor based on existing Tensorx = x.new_ones(5, 3, dtype=torch.double) # Create new Tensor based on existing Tensorx = torch.randn_like(x, dtype=torch.float) # Obtain matrix size, return torch.Size supports tuple types, all tuple operationsx.size() Tensor Operation 1234567891011# Operation 1y = torch.rand(5, 3) print(x + y)# Operation 2print(torch.add(x, y)) result = torch.empty(5, 3)# Operation 3torch.add(x, y, out=result) print(result) Change Tensor shape and convert to numpy array 1234567891011121314151617# When changing size and shape of sensor, use torch.viewx = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# If only one value exists in the tensor, a numeric value can be obtained# using .item()print(x.item()) # Convert torch tensor to numpy arraya = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor Change 123456789101112131415import torchimport numpy as npx = torch.randn(1)# Run only in CUDA-enabled environments (GPU environments)if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # Create a direct tensor on a GPU y = torch.ones_like(x, device=device) # Change CPU Tensor to GPU Tensor x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/en/Pytorch-Tensor/"},{"title":"Pytorch Autograd","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Autogard autogard package provides automatic differentiation for all operations of the Tensor. Setting the .requires_gard property to True in the torch.Tensor class tracks all operations performed on that sensor. After the calculation is complete, you can call .backward() to automatically calculate the gradient To stop Tensor from tracking records, you can call .detach() to separate from the computational history Code blocks can be wrapped with torch.no_gard(): to avoid historical tracking and memory usage Useful for evaluating models with learnable parameters, set to require_guard=True, although gradients are not required. Function Class Tensor and Function are interconnected and encode all computational processes to create non-circulating graphs Each Tensor has a .gard_fn property, which refers to the Function that generated the Tensor (The gradient_fn of the user-generated Tensor is None) The derivative calculation calls the .backward() of the Tensor Backward does not require factors if Tensor is Scally, but when it has multiple elements, it is necessary to shape the Tensor as a factor of gradient.","link":"/2021/01/25/en/Pytorch-Autograd/"},{"title":"Understanding CNN","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. CNN? Cnovolutional Neural NetworkExtracting the features of the input image through the convolutional layer andperforming classification based on them Convolutional Layer Filter (Kernel) to extract features, configured as an Activation Functionto replace the value of the filter with a nonlinear value Filter doesn’t look at the whole thing, It looks at some parts.","link":"/2021/01/27/en/Understanding-CNN/"},{"title":"Pytorch Autograd 실습","text":"This article is also available in English. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import torchx = torch.ones(2, 2, requires_grad=True) # 연산을 기록print(x)y = x + 2 # 연산 수행print(y)print(y.grad_fn) # 연산 결과이므로 grad_fn을 가짐z = y*y*3 # z = (x+2)^2*3out = z.mean()print(z, out)&quot;&quot;&quot;.requires_gard_(...)는 기존 Tensor의 requires_grad 값을 inplace 방식으로 변경.입력값 지정되지 않으면 default는 False&quot;&quot;&quot;a = torch.randn(2, 2) # 기본 requires_gard = Falsea = ((a * 3) / (a - 1))print(a.requires_grad) # Falsea.requires_grad_(True) # _가 뒤에 붙어있기 때문에 inplace 연산print(a.requires_grad) # Trueb = (a * a).sum() # True의 연산 결과이므로 grad_fn 가짐print(b.grad_fn)print(b)# 역전파(backprop)&quot;&quot;&quot;1. scalar 값의 backprop = 미분 결과에 해당 값을 넣었을 때.x = 2행 2열짜리 1이 들어있는 행렬y = x+2 == 2행 2열짜리 3이 들어있는 행렬z = y*y*3 == 2행 2열 27이 들어있는 행렬 (3*(x+2)^2)out = z의 평균&quot;&quot;&quot;out.backward() # == out.backward(torch.tensor(1.))print(out)print(x.grad)# x가 사용된 최종 연산식에서 x에대한 미분(out = 3*(x+2)^2/4 -&gt; 3*(x+2)/2)# (x = 1) (x = 1은 out.backward()에서 설정됨)&quot;&quot;&quot;2. vector의 backprop = Jacobian Matrix (m차원에서 n차원으로 가는 함수가 있다고 할 때 각각 차원에 대해 모든 편미분 값을 모아놓은 matrix) 일반적인 미분 : 1변수 함수만 다룸 편미분 : 다변수 함수에서 한 변수만 변수로, 나머지 변수는 상수로 vector의 backprop은 편미분값 matrix에 곱해주는 어떤 값&quot;&quot;&quot;x = torch.rand(3, requires_grad=True)y = x * 2 # 2xwhile y.data.norm() &lt; 1000: y = y * 2 # 2^n*xprint(y)v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v) # 2^n에 0.1, 1.0, 0.0001 곱해준 값print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): # Tensor 연산 기록 멈춤 print((x ** 2).requires_grad)print(x.requires_grad)y = x.detach() # content는 같지만 require_grad가 False인 새로운 Tensor 생성print(y.requires_grad)print(x.eq(y).all())","link":"/2021/01/26/kr/Pytorch-Autograd-Practice/"},{"title":"머신 러닝 (Machine Learning)","text":"This article is also available in English. 머신러닝 데이터를 기반으로 패턴을 학습, 결과를 예측하는 알고리즘 기법개발자가 데이터, 업무 로직의 특성을 직접 감안한 프로그램을 만들 경우 난이도, 개발복잡도가 너무 높아질 수밖에 없는 분야에 유용 머신러닝의 분류 지도학습 분류 회귀 추천 시스템 시각, 음성 인지 텍스트 분석, NLP 등 비지도학습 클러스터링 차원 축소 강화 학습 등 머신러닝의 최대 단점 데이터 의존적좋은 품질의 데이터를 갖추지 못하면 머신러닝 수행 결과도 좋을 수 없음최적의 머신러닝 알고리즘, 모델 파라미터를 구축하는 능력도 중요하지만 데이터를 효율적으로 가공, 처리, 추출하는 능력 또한 중요","link":"/2021/01/27/kr/Machine-Learning/"},{"title":"Pytorch Autograd","text":"This article is also available in English. Autogard (자동 미분) autogard 패키지는 Tensor의 모든 연산에 대한 자동 미분 제공 torch.Tensor 클래스에서 .requires_gard 속성을 True로 설정하면 해당 tensor에서 이뤄진 모든 연산들을 추적(track) 계산이 완료된 후 .backward()**를 호출하여 **변화도(gradient)를 자동으로 계산할 수 있음 Tensor가 기록 추적하는 것을 중단시키려면 .detach()**를 호출하여 연산 기록으로부터 분리할 수 있음 기록 추적 및 메모리 사용을 방지하기 위해 코드 블럭을 **with torch.no_gard(): 로 감쌀 수 있음 변화도(gradient)는 필요 없지만 requires_guard=True로 설정되어 학습 가능한 매개변수를 갖는 모델 평가 시 유용 Function 클래스 Tensor와 Function은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프를 생성 각 Tensor는 .gard_fn 속성을 갖는데, 이는 Tensor를 생성한 Function을 참고함 (사용자 생성 Tensor의 grad_fn은 None) 도함수 계산은 Tensor의 .backward() 호출 Tensor가 스칼리인 경우 backward에 인자 필요 없으나 여러 요소를 가질 때는 tensor의 모양을 gradient의 인자로 지정할 필요가 있음","link":"/2021/01/25/kr/Pytorch-Autograd/"},{"title":"Pytorch 신경망","text":"This article is also available in English. 신경망 torch.nn 패키지를 사용하여 생성 가능nn은 모델을 정의하고 미분하는데 autograd 사용nn.Module은 layer와 output을 반환하는 forward(input) 메서드 포함 신경망의 일반적인 학습 과정 학습 가능한 매개변수(또는 가중치(weight))를 갖는 신경망 정의 데이터셋 입력 반복 입력을 신경망에서 전파 손실(loss) 계산 변화도(gradient)를 신경망 매개변수들에 역전파 신경망의 가중치 갱신(새로운 가중치 = 기존 가중치 - 학습률(learning_rate) * 변화도(gradient) 신경망 정의 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() &quot;&quot;&quot; 입력 이미지 채널 1개, 출력 채널 6개, 3x3의 정사각 컨볼루션 행렬 컨볼루션 커널 정의 troch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int) : input image의 channel수. rgb면 3 out_channels(int) : convolution에 의해 생성된 channel 수 kernel_size(int or tuple) : convoling_kernel 크기. (filter) stride(int or tuple) : convolution의 stride를 얼만큼 줄 것인가. default는 1, stride는 이미지 횡단 시 커널의 스텝 사이즈 padding(int or tuple) : zero padding을 input 양쪽 인자만큼. default는 0이라서 기본적으로 설정하지 않을 경우 zero padding 적용하지 않음 self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) 아핀(affine) 연산: y = Wx + b self.fc1 = nn.Linear(16*6*6, 120) # 6*6은 이미지 차원 self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) &quot;&quot;&quot; def forward(self, x): # (2, 2) 크기 윈도우에 대해 맥스 풀링(max pooling) x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 크기가 제곱수라면 하나의 숫자만을 특정 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # 배치 차원을 제외한 모든 차원 num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 2D convolution using a kernel size of 3, stride of 1 and padding forward 함수만 정의하면, backward 함수는 autograd를 사용하여 자동으로 정의됨.forward 함수에서는 어떠한 Tensor 연산을 사용해도 됨모델의 학습 가능 매개변수는 net.parameters()에 의해 반환됨 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768params = list(net.parameters())print(len(params))print(params[0].size())# 임의의 32x32값 입력input = torch.randn(1, 1, 32, 32)out = net(input)print(out)# 모든 매개변수의 변화도 버퍼를 0으로 설정, 무작위 값으로 역전파net.zero_grad()out.backward(torch.rand(1, 10))# 손실 함수# output, target을 한 쌍의 입력으로 받아, 출력이 정답으로부터 얼마나 멀리 떨어져있는지# 추정하는 값을 계산# 간단한 손실 함수로 출력과 대상 간 평균제곱오차를 계산하는 nn.MSEloss가 있음output = net(input)target = torch.randn(10) # 비교를 위한 예시, 임의의 정답target = target.view(1, -1) # 출력과 같은 shape로 변환criterion = nn.MSELoss()loss = criterion(output, target)print(loss)&quot;&quot;&quot;input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss&quot;&quot;&quot;# 변화도가 누적된 .grad Tensorprint(loss.grad_fn)print(loss.grad_fn.next_functions[0][0])print(loss.grad_fn.next_functions[0][0].next_functions[0][0])# 역전파# 기존 변화도를 없애지 않으면 기존의 변화도에 누적됨net.zero_grad()print('conv1.bias.grad before backward') # 0으로 초기화되어있음print(net.conv1.bias.grad)loss.backward() # loss = criterion(output, target)print('conv1.bias.grad after backward')print(net.conv1.bias.grad)# 가중치 갱신# 확률적 경사하강법learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate)# torch.optim이라는 작은 패키지에 SGD, Nesterov-SGD, Adam, RMSProp등# 다양한 갱신 규칙을 구현해두었음import torch.optim as optim# Optimizer 생성optimizer = optim.SGD(net.parameters(), lr=0.01)# 학습 과정# optimizer.zero_grad()를 사용하여 수동으로 변화도 버퍼를 0으로 설정optimizer.zero_grad()output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()","link":"/2021/01/26/kr/Pytorch-Neural-Network/"},{"title":"Pytorch 텐서 생성, 연산, 변환","text":"This article is also available in English. Pytorch란? 딥러닝 프로젝트 빌드를 위해 도움을 주는 파이썬 라이브러리코어 데이터 구조인 텐서를 제공 (numpy 배열과 유사한 다차원 배열)텐서는 수학적 연산을 가속화 (GPU 사용 가능) 성능적인 이유로 대부분 C++과 CUDA로 만들어져있음. 설치방법 PyTorch 설치 링크 본인의 환경에 맞게 설정 후 Run this Command 명령어 복사 GPU 확인 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Tensor 생성 123456789101112131415# 그 시점에 할당된 메모리에 존재하던 값들이 초기값으로 나타남x = torch.empty(5, 3)# 무작위로 초기화된 행렬 (0 &lt;= x &lt; 1)x = torch.rand(5, 3) # dtype = long, 0으로 채워진 행렬x = torch.zeros(5, 3, dtype=torch.long) # list를 사용하여 직접 tensor 생성x = torch.tensor([5.5, 3]) # 기존 tensor를 바탕으로 새로운 tensor 생성x = x.new_ones(5, 3, dtype=torch.double) # 기존 tensor를 바탕으로 새로운 tensor 생성x = torch.randn_like(x, dtype=torch.float) # 행렬 크기 구하기, 반환인 torch.Size는 튜플 타입, 모든 튜플 연산 지원x.size() Tensor 연산 1234567891011# 연산 문법 1y = torch.rand(5, 3) print(x + y)# 연산 문법 2print(torch.add(x, y)) result = torch.empty(5, 3)# 연산 문법 3torch.add(x, y, out=result) print(result) Tensor shape 변경 및 numpy 배열로 변환 12345678910111213141516# tensor의 size, shape 변경 시에는 torch.view 사용x = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# tensor에 하나의 값만 존재한다면 .item()을 사용하여 숫자 값 얻을 수 있음print(x.item()) # torch tensor를 numpy 배열로 변환a = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor 변환 123456789101112131415import torchimport numpy as npx = torch.randn(1)# CUDA 사용 가능 환경(GPU 환경)에서만 실행if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # GPU 상 직접 tensor 생성 y = torch.ones_like(x, device=device) # CPU 텐서를 GPU 텐서로 변경 x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/kr/Pytorch-Tensor/"},{"title":"CNN에 대한 이해","text":"This article is also available in English. CNN? Cnovolutional Neural Network컨볼루셔널 계층을 통해 입력 이미지의 특징(Feature)를 추출, 이를 기반으로 분류를 수행 Convolutional Layer 특징을 추출하는 Filter(Kernel), 필터의 값을 비선형 값으로 바꾸는 Activation Function으로 구성 Filter는 전체를 보는 것이 아닌 일부분을 집중해서 본다","link":"/2021/01/27/kr/Understanding-CNN/"},{"title":"Scikit-Learn-1 (Classification Example)","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. scikit-learn The most popular Python machine learning library Classifying varieties with brush flower data setPrediction of flower varieties based on petal length, width, flower support length, and width Classification One of the most representative supervised learning methods Practice Decision Tree algorithms 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pandas as pdiris = load_iris() # Load iris datasetiris_data = iris.data # numpy data consisting of only input featuresiris_label = iris.target # Prediction labelprint('iris target값:', iris_label)print('iris target명:', iris.target_names)iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) # make dataframeiris_df['label'] = iris.targetprint(iris_df.head(3))# Learn data, test data separation, feature x, label y,# 80% learning data, 20% testing datax_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=0.2, random_state=1)# Creating a DecisionTreeClassifier objectdt_clf = DecisionTreeClassifier(random_state=1)dt_clf.fit(x_train, y_train) # Model Learningpred = dt_clf.predict(x_test) # Model Predictionfrom sklearn.metrics import accuracy_score # Get Accuracyprint('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))&quot;&quot;&quot;1. Separation of data sets (learning data, test data separation)2. Model Learning3. Perform predictions4. Evaluation&quot;&quot;&quot;","link":"/2021/01/27/en/Scikit-Learn-1/"},{"title":"Scikit-Learn-1 (분류 에제)","text":"This article is also available in English. scikit-learn 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 붓꽃 데이터 세트로 붓꽃의 품종 분류하기꽃잎 길이, 너비, 꽃받침 길이, 너비를 기반으로 꽃의 품종 예측 분류(Classification) 대표적인 지도학습 방법의 하나 의사 결정 트리(Decision Tree) 알고리즘 실습 12345678910111213141516171819202122232425262728293031323334353637from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pandas as pdiris = load_iris() # 붓꽃 데이터 세트 로드iris_data = iris.data # 입력 피처만으로 된 numpy 데이터iris_label = iris.target # 예측 labelprint('iris target값:', iris_label)print('iris target명:', iris.target_names)iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) # dataframe 생성iris_df['label'] = iris.targetprint(iris_df.head(3))# 학습 데이터, 테스트 데이터 분리, feature들을 x, label을 y로,# 80% 학습 데이터, 20% 테스트 데이터x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=0.2, random_state=1)dt_clf = DecisionTreeClassifier(random_state=1) # DecisionTreeClassifier 객체 생성dt_clf.fit(x_train, y_train) # 모델 학습pred = dt_clf.predict(x_test) # 모델 예측from sklearn.metrics import accuracy_score # 정확도 파악print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))&quot;&quot;&quot;1. 데이터 세트 분리 (학습 데이터, 테스트 데이터 분리)2. 모델 학습3. 예측 수행4. 평가&quot;&quot;&quot;","link":"/2021/01/27/kr/Scikit-Learn-1/"},{"title":"Scikit-Learn-2","text":"This article is also available in English. Estimator 이해 scikit-learn에서 분류 알고리즘을 Classifier로, 회귀 알고리즘을 Regressor로 지칭Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름모델 학습은 fit(), 학습된 모델을 사용한 예측은 predict() 메소드 제공 cross_val_score()와 같은 evaluation 함수, GridSearchCV와 같은 하이퍼파라미터 튜닝을 지원하는 클래스의 경우 Estimator를 인자로 받음 비지도학습인 차원 축소, 클러스터링, 피처 추출 등을 구현한 클래스 역시 대부분 fit(), transform() 적용비지도학습과 피처 추출에서 fit()은 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 구조 작업에 해당fit()으로 구조를 맞추면 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출 등 실제 작업은 transform()으로 수행두 기능을 결합한 fit_transform()도 함께 제공 scikit-learn 주요 패키지 sklearn.datasets datasets.make_classificiation분류를 위한 데이터 세트 생성높은 상관도, 불필요한 속성 등 노이즈 효과를 위한 데이터를 무작위로 생성 datasets.make_bolbs()클러스터링을 위한 데이터 세트 무작위 생성군집 지정 개수에 따라 여러가지 클러스터링을 위한 데이터세트 생성 사이킷런 데이터세트는 딕셔너리 형태로 되어있음 data : 피처의 데이터세트 target : 분류 시 레이블 값, 회귀 시 숫자 결과값 target_names : 개별 레이블 이름 feature_names : 피처 이름 DESCR : 데이터 세트에 대한 설명, 각 피처 설명 data, target은 넘파이 배열, target_names, feature_names는 넘파이 배열 또는 list 타입DESCR은 스트링 타입","link":"/2021/01/27/kr/Scikit-Learn-2/"},{"title":"Scikit-Learn-3 (Model Selection)","text":"This article is also available in English. Model Selection 모듈 학습 데이터, 테스트 데이터를 분리하거나 교차 검증 분할 및 평가, Estimator의 하이퍼 파라미터 튜닝 train_test_split() test_size : 테스트 데이터의 크기 shuffle : 데이터를 미리 섞을지. default는 True random_state : 호출 시 마다 동일한 학습, 데이터세트를 생성하기 위한 난수값 반환값은 튜플 형태 교차 검증 학습, 테스트 데이터를 나누는 것만으로는 과적합(Overfitting)에 취약과적합 : 모델이 학습 데이터에만 과도하게 최적화, 실제 예측 성능이 과도하게 떨어짐별도의 여러 세트로 구성된 학습 데이터와 검증 데이터에서 평가 수행 교차 검증 기반 1차 평가 이후 최종적으로 테스트 데이터에 적용하여 평가하는것이 일반적 K-fold 교차 검증 K개의 데이터 폴드 세트를 만들어 K번만큼 각 폴드 세트에 검증 평가를 반복적으로 수행 1234567891011121314151617181920212223242526272829303132333435from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFoldfrom sklearn.datasets import load_irisimport numpy as npiris = load_iris()features = iris.datalabel = iris.targetdt_clf = DecisionTreeClassifier(random_state=156)# 5개의 폴드 생성kfold = KFold(n_splits=5)cv_accuracy = []print(&quot;붓꽃 데이터 크기 :&quot;, features.shape[0])n_iter = 0# KFold 객체의 split()은 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환for train_index, test_index in kfold.split(features): X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size)) print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index)) cv_accuracy.append(accuracy)print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy)) Stratified K fold 불균형한 분포를 갖는 레이블 데이터 집합을 위한 K 폴드 방식.특정 레이블 값이 특이하게 많거나 적어서 값의 분포가 치우친 것 12345678910111213141516171819202122232425dt_clf = DecisionTreeClassifier(random_state=156)features = iris.datalabel = iris.targetskfold = StratifiedKFold(n_splits=3)n_iter=0cv_accuracy = []for train_index, test_index in skfold.split(features, label): X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size)) print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index)) cv_accuracy.append(accuracy)print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))print('## 평균 검증 정확도:', np.mean(cv_accuracy)) cross_val_score() 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API KFold의 데이터 학습, 예측 코드 폴드 세트 지정 for 루프에서 반복 학습, 테스트 데이터 인덱스 추출 반복적으로 학습, 예측 수행 cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pred_dispatch=’2*n_jobs’) estimator : Classifier 또는 Regression X : 피처 데이터세트 y : 레이블 데이터세트 scoring : 예측 성능 평가 지표 cv : 교차 검증 폴드 수 반환값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환 cross_validate()는 여러 평가 지표를 반환할 수 있음 1234567891011121314from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import cross_val_score, cross_validatefrom sklearn.datasets import load_irisimport numpy as npiris_data = load_iris()dt_clf = DecisionTreeClassifier(random_state=156)data = iris_data.datalabel = iris_data.targetscores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)print(&quot;교차 검증 정확도:&quot;, np.round(scores, 4))print(&quot;평균 검증 정확도:&quot;, np.round(np.mean(scores), 4)) GridSearchCV 교차 검증, 최적 하이퍼 파라미터 튜닝하이퍼 파라미터 : 머신러닝 알고리즘을 구성하는 주요 구성 요소이 값을 조정해 알고리즘의 예측 성능 개선 가능 교차 검증 기반으로 하이퍼 파라미터의 최적 값을 찾게 해줌 파라미터 estimator param_grid : key + 리스트 값을 갖는 딕셔너리. estimator 튜닝을 위한 파라미터 명, 파라미터 값 지정 scoring : 예측 성능 평가 지표 cv : 교차 검증 폴드 수 refit : 가장 최적의 하이퍼파라미터 찾은 뒤 estimator 객체를 해당 하이퍼파라미터로 재학습시킴 (default = True) 123456789101112131415161718192021222324252627from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_splitfrom sklearn.metrics import accuracy_scoreiris = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=121)dtree = DecisionTreeClassifier(random_state=121)parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2, 3]}import pandas as pdgrid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)grid_dtree.fit(X_train, y_train)scores_df = pd.DataFrame(grid_dtree.cv_results_)print(scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']])print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)print('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_dtree.best_score_))estimator = grid_dtree.best_estimator_pred = estimator.predict(X_test)print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))","link":"/2021/02/02/kr/Scikit-Learn-3/"},{"title":"Scikit-Learn-3 (Model Selection)","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Model Selection Module Separate learning data, test data, or split and evaluate cross-validation, tuning the estimator’s hyperparameters train_test_split() test_size : Size of test data shuffle : Settings for pre-mixing data. default is True random_state : Random number of values to generate the same learning, dataset for each call Return values are in tuple shape Cross Validation Vulnerable to overfitting just by dividing learning, test dataOverfitting: Model over-optimizes to learning data only, resulting in excessive prediction performancePerform evaluations on training and validation data consisting of multiple separate sets After a cross-validation-based primary evaluation, it is common to finally apply to test data for evaluation. K-fold Cross Validation Create a set of K datafolds to repeatedly evaluate verification on each fold set as many times as K times. 1234567891011121314151617181920212223242526272829303132333435from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFoldfrom sklearn.datasets import load_irisimport numpy as npiris = load_iris()features = iris.datalabel = iris.targetdt_clf = DecisionTreeClassifier(random_state=156)# Create 5 Foldskfold = KFold(n_splits=5)cv_accuracy = []print(&quot;붓꽃 데이터 크기 :&quot;, features.shape[0])n_iter = 0# split() of the KFold object returns the low index of the test for per-fold learning and validation as arrayfor train_index, test_index in kfold.split(features): X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#{0} Cross-validation accuracy :{1}, train data size: {2}, validation data size: {3}'.format(n_iter, accuracy, train_size, test_size)) print('#{0} index of validation set: {1}'.format(n_iter, test_index)) cv_accuracy.append(accuracy)print('\\n## Average accuracy of validation:', np.mean(cv_accuracy)) Stratified K fold K-fold scheme for label datasets with unbalanced distributions.The distribution of a particular label is skewed due to its unusually high or low value. 12345678910111213141516171819202122232425dt_clf = DecisionTreeClassifier(random_state=156)features = iris.datalabel = iris.targetskfold = StratifiedKFold(n_splits=3)n_iter=0cv_accuracy = []for train_index, test_index in skfold.split(features, label): X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#{0} Cross-validation accuracy :{1}, train data size: {2}, validation data size: {3}'.format(n_iter, accuracy, train_size, test_size)) print('#{0} index of validation set: {1}'.format(n_iter, test_index)) cv_accuracy.append(accuracy)print('\\n## Accuracy by cross validation:', np.round(cv_accuracy, 4))print('## Average accuracy of validation:', np.mean(cv_accuracy)) cross_val_score() API that make cross-validation more convenient KFold’s Data Learning, Predictive Code Specify Fold Set for loop learning iterations, extracting test data indexes Learning and making predictions repeatedly cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pred_dispatch=’2*n_jobs’) estimator : Classifier or Regression X : Feature Dataset y : Label Dataset scoring : Predictive Performance Evaluation Indicators cv : Number of cross-validation folds Returns the performance indicator measurement specified by the scoring parameter in array form cross_validate() can return multiple evaluation metrics 1234567891011121314from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import cross_val_score, cross_validatefrom sklearn.datasets import load_irisimport numpy as npiris_data = load_iris()dt_clf = DecisionTreeClassifier(random_state=156)data = iris_data.datalabel = iris_data.targetscores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)print(&quot;Cross-validation accuracy:&quot;, np.round(scores, 4))print(&quot;Average accuracy of validation:&quot;, np.round(np.mean(scores), 4)) GridSearchCV cross-validation, tuning the optimal hyperparametersHyperparameters: the main components of machine learning algorithmsYou can adjust this value to improve the predictive performance of the algorithm. Allows you to find the optimal value of the hyperparameters based on cross-validation Parameters estimator param_grid : Dictionary that key + list value. parameter name, parameter value specification for estimator tuning scoring : Predictive Performance Evaluation Indicators cv : Number of cross-validation folds refit : Find the most optimal hyperparameter and re-educate the estimator object to that hyperparameter (default = True) 123456789101112131415161718192021222324252627from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_splitfrom sklearn.metrics import accuracy_scoreiris = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=121)dtree = DecisionTreeClassifier(random_state=121)parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2, 3]}import pandas as pdgrid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)grid_dtree.fit(X_train, y_train)scores_df = pd.DataFrame(grid_dtree.cv_results_)print(scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']])print('GridSearchCV Optimal Parameters:', grid_dtree.best_params_)print('GridSearchCV Highest Accuracy:{0:.4f}'.format(grid_dtree.best_score_))estimator = grid_dtree.best_estimator_pred = estimator.predict(X_test)print('Test Dataset Accuracy: {0:.4f}'.format(accuracy_score(y_test, pred)))","link":"/2021/02/02/en/Scikit-Learn-3/"},{"title":"Scikit-Learn-4 (데이터 전처리)","text":"This article is also available in English. 데이터 전처리 (Data Preprocessing) ML 알고리즘 적용 전 데이터에 대해 미리 처리해야 할 기본 사항 Nan, Null값고정된 다른 값으로 변환해야 함얼마 되지 않는다면 피처의 평균값 등으로 대체할 수 있음Null값이 대부분이라면 피처를 드롭하는 것이 더 좋음 문자열입력 값으로 허용하지 않음인코딩을 통해 숫자 형으로 변환해야 함카테고리형, 텍스트형 피처로 구분텍스트형 피처는 피처 벡터화 등의 기법으로 벡터화, 불필요하다면 드롭 데이터 인코딩 레이블 인코딩카테고리 피처를 코드형 숫자 값으로 변환 123456789101112from sklearn.preprocessing import LabelEncoderitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)print('인코딩 변환값:', labels)print('인코딩 클래스:', encoder.classes_)print('디코딩 원본 값:', encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) 원-핫 인코딩피처 값의 유형에 따라 새로운 피처를 추가고유 값에 해당하는 컬럼에만 1 표시, 나머지 컬럼에는 0 표시 1234567891011121314151617181920212223242526from sklearn.preprocessing import OneHotEncoder, LabelEncoderimport numpy as npitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)labels = labels.reshape(-1, 1)print(labels)oh_encoder = OneHotEncoder()oh_encoder.fit(labels)oh_labels = oh_encoder.transform(labels)print('원-핫 인코딩 데이터')print(oh_labels.toarray())print(&quot;원-핫 인코더 데이터 차원&quot;)print(oh_labels.shape)# Pandas OneHotEncoding APIimport pandas as pddf = pd.DataFrame({'item':items})print(pd.get_dummies(df)) 피처 스케일링과 정규화 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업표준화, 정규화 표준화(Standardization)평균 0, 분산 1인 가우시안 정규 분포를 갖는 값으로 변환 1234567891011121314151617181920212223from sklearn.datasets import load_irisimport pandas as pdiris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)print('feature들의 평균')print(iris_df.mean())print('\\nfeature들의 분산')print(iris_df.var())from sklearn.preprocessing import StandardScaler # 평균 0, 분산 1에 가깝게scaler = StandardScaler()scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature들의 평균')print(iris_df_scaled.mean())print('\\nfeature들의 분산')print(iris_df_scaled.var()) 정규화(Normalization)서로 다른 피처 크기를 통일시키기 위해 크기를 변환ex) MinMaxScaler 123456789101112131415161718from sklearn.datasets import load_irisimport pandas as pdiris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature들의 최솟값')print(iris_df_scaled.min())print('\\nfeature들의 최대값')print(iris_df_scaled.max())","link":"/2021/02/02/kr/Scikit-Learn-4/"},{"title":"Scikit-Learn-4 (Preprocessing))","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Data Preprocessing Basics to preprocess data before applying ML algorithms Nan, NullNeed to convert to another fixed valueIf it’s not long, it can be replaced by the average value of the feature.It is better to drop features if most null values are Null StringDo not allow as input valueMust be converted to numeric via encodingCategorized by Category, Text FeatureText-type features are vectorized by techniques such as feature vectorization, and drop if necessary. Data Encoding Label EncodingConverting category features to numeric values 123456789101112from sklearn.preprocessing import LabelEncoderitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)print('Encoding conversion value:', labels)print('Encoding Class:', encoder.classes_)print('Decode Source Value:', encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) One-Hot EncodingAdd new features according to the type of feature valueShow 1 only in columns corresponding to unique values, 0 in remaining columns 1234567891011121314151617181920212223242526from sklearn.preprocessing import OneHotEncoder, LabelEncoderimport numpy as npitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)labels = labels.reshape(-1, 1)print(labels)oh_encoder = OneHotEncoder()oh_encoder.fit(labels)oh_labels = oh_encoder.transform(labels)print('One-Hot Encoded Data')print(oh_labels.toarray())print(&quot;One-Hot Encoded Data Dimensions&quot;)print(oh_labels.shape)# Pandas OneHotEncoding APIimport pandas as pddf = pd.DataFrame({'item':items})print(pd.get_dummies(df)) Feature scaling and normalization To set a constant range of values for different variablesStandardization, Normalization StandardizationConvert to a value with a Gaussian normal distribution with mean 0, variance 1 1234567891011121314151617181920212223from sklearn.datasets import load_irisimport pandas as pdiris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)print('Average of the features')print(iris_df.mean())print('\\nVariance of the features')print(iris_df.var())from sklearn.preprocessing import StandardScaler # 평균 0, 분산 1에 가깝게scaler = StandardScaler()scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('Average of the features')print(iris_df_scaled.mean())print('\\nVariance of the features')print(iris_df_scaled.var()) NormalizationConvert sizes to unify different feature sizesex) MinMaxScaler 123456789101112131415161718from sklearn.datasets import load_irisimport pandas as pdiris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('Minimum Value of the features')print(iris_df_scaled.min())print('\\nMaximum Value of the features')print(iris_df_scaled.max())","link":"/2021/02/02/en/Scikit-Learn-4/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"Scikit learn","slug":"Scikit-learn","link":"/tags/Scikit-learn/"},{"name":"Convolutional Neural Network","slug":"Convolutional-Neural-Network","link":"/tags/Convolutional-Neural-Network/"},{"name":"파이토치","slug":"파이토치","link":"/tags/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"머신 러닝","slug":"머신-러닝","link":"/tags/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"사이킷 런","slug":"사이킷-런","link":"/tags/%EC%82%AC%EC%9D%B4%ED%82%B7-%EB%9F%B0/"},{"name":"분류","slug":"분류","link":"/tags/%EB%B6%84%EB%A5%98/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Pytorch","slug":"Pytorch","link":"/categories/Pytorch/"}]}
{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Machine Learning","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Machine Learning Algorithm techniques for learning patterns based on data and predicting resultsWhen developers create programs that take into account the nature of their data and work logic, useful for areas where difficulty and complexity of development are inevitably too high The field of machine learning Supervised Learning Classification Regression Recommanded System Visual, Voice Recognition Text analysis, NLP, etc. Unsupervised Learning Clustering Dimensionality Reduction Reinforcement learning, etc. The biggest drawback of machine learning Data-dependentMachine learning results can’t be good without good quality dataThe ability to build optimal machine learning algorithms and model parameters is important, but the ability to process, process, and extract data efficiently is also important.","link":"/2021/01/27/en/Machine-Learning/"},{"title":"Pytorch Neural Network","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Neural Network Can be created using the torch.nn packagenn uses autograd to define and differentiate modelsnn.Module contains forward(input) method that returns layer and output Typical Learning Process of Neural Networks Define a neural network with learnable parameters (or weights) Repeat Dataset Input Propagate inputs from neural networks Calculating loss Backprop the gradient to the neural network parameters. Updating the weight of a neural networks(New Weight = Existing Weight - Learning_rate * Gradient) Define Neural Network 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() &quot;&quot;&quot; 1 input image channel, 6 output channels, 3x3 square convolution matrix Convolutional Kernel Definitions troch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int) : Number of channel about input image. if rgb == 3 out_channels(int) : Number of channels created by convolution kernel_size(int or tuple) : convoling_kernel size. (filter) stride(int or tuple) : Stride size of convolution default is 1, stride is the step size of the kernel when traversing the image padding(int or tuple) : zero padding size Default is 0 so zero padding is not applied if not set by default self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) Affine operation : y=Wx+b self.fc1 = nn.Linear(16*6*6, 120) # 6*6 is image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) &quot;&quot;&quot; def forward(self, x): # Max pooling for (2, 2) size windows x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square number, specify only one number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # All dimensions except batch dimensions num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 2D convolution using a kernel size of 3, stride of 1 and padding If you define a forward function only, the backward function is automatically defined using autograd.You can use any Tensor operation in the forward function.The model’s learnable parameters are returned by net.parameters() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071params = list(net.parameters())print(len(params))print(params[0].size())# Enter any 32x32 valueinput = torch.randn(1, 1, 32, 32)out = net(input)print(out)# Set all parameters to zero degrees of change buffer,# backpropagating to random valuesnet.zero_grad()out.backward(torch.rand(1, 10))# loss function# Take output, target as a pair of inputs and calculate the estimate of# how far the output is from the correct answer.# nn.MSEloss calculates mean square error between output and target# as a simple loss functionoutput = net(input)target = torch.randn(10) # Example for comparison, random correct answertarget = target.view(1, -1) # Convert to same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target)print(loss)&quot;&quot;&quot;input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss&quot;&quot;&quot;# .grad Tensor with accumulated variabilityprint(loss.grad_fn)print(loss.grad_fn.next_functions[0][0])print(loss.grad_fn.next_functions[0][0].next_functions[0][0])# backprop# If you don't eliminate the traditional changes,# they accumulate in the existing ones.net.zero_grad()print('conv1.bias.grad before backward') # Initialized to 0print(net.conv1.bias.grad)loss.backward() # loss = criterion(output, target)print('conv1.bias.grad after backward')print(net.conv1.bias.grad)# Updating Weight# Stochastic gradient descent(SGD)learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate)# implemented various renewal rules such as SGD, Nesterov-SGD, Adam,and RMSProp# in a small package called torch.optimimport torch.optim as optim# Create Optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# Training process# Manually set the change chart buffer to zero using optimizer.zero_grad()optimizer.zero_grad()output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()","link":"/2021/01/26/en/Pytorch-Neural-Network/"},{"title":"Scikit-Learn-2","text":"","link":"/2021/01/27/en/Scikit-Learn-2/"},{"title":"Pytorch Autograd Practice","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchx = torch.ones(2, 2, requires_grad=True) # Record operationsprint(x)y = x + 2 # Performing operationsprint(y)print(y.grad_fn) # This is the result of the operation, so it has grad_fn.z = y*y*3 # z = (x+2)^2*3out = z.mean()print(z, out)&quot;&quot;&quot;.requires_gard_(...) changes the requirements_grad value of the existing Tensor toinplace. If no input value is specified, default is False&quot;&quot;&quot;a = torch.randn(2, 2) # default requires_gard = Falsea = ((a * 3) / (a - 1))print(a.requires_grad) # Falsea.requires_grad_(True) # Operate inplace because _ is attached to the behindprint(a.requires_grad) # Trueb = (a * a).sum() # Grad_fn because it is the result of an operation of Trueprint(b.grad_fn)print(b)# backprop&quot;&quot;&quot;1. backprop of scalar value = When the value is added to the differential result.x = a matrix containing two lines, two rows, 1y = x+2 == a matrix containing two-line, two-row, 3z = y*y*3 == a matrix containing two-line, two-row, 27 (3*(x+2)^2)out = Mean of z&quot;&quot;&quot;out.backward() # == out.backward(torch.tensor(1.))print(out)print(x.grad)# The differential for x in the final equation in which x was used.# (out = 3*(x+2)^2/4 -&gt; 3*(x+2)/2)# (x = 1) (x = 1 is set in out.backward())&quot;&quot;&quot;2. backprop of vector value = Jacobian Matrix (a matrix of all partial differential values for each dimension, given that there is a function from m to n.) Typical differential: deals with only one-variable functions Partial differential: In multivariate functions, only one variable is a variable and the other is a constant. The vector's backprop is any value multiplied by the partial differential matrix.&quot;&quot;&quot;x = torch.rand(3, requires_grad=True)y = x * 2 # 2xwhile y.data.norm() &lt; 1000: y = y * 2 # 2^n*xprint(y)v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v) # 2^n multiplied by 0.1, 1.0, 0.0001print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): # Stop Recording Tensor Operations print((x ** 2).requires_grad)print(x.requires_grad)y = x.detach() # Create a new Tensor with the same content but false require_gradprint(y.requires_grad)print(x.eq(y).all())","link":"/2021/01/26/en/Pytorch-Autograd-Practice/"},{"title":"Create, Operate, Convert Pytorch Tensor","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Pytorch? Python Library Helps Build Deep Learning ProjectsProvides a tensor, a core data structure (multi-dimensional array similar to the numpy array)Tensor accelerates mathematical operations (GPU available) It is mostly made of C++ and CUDA for performance reasons. Installation PyTorch Installation Link Copy Run this Command after setting it to your environment. Check GPU 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Create Tensor 12345678910111213141516# The values that existed in the memory allocated at that time appear as# initial values.x = torch.empty(5, 3)# Randomly initialized matrix (0 = x &lt; 1)x = torch.rand(5, 3) # dtype = long, Matrix filled with zerosx = torch.zeros(5, 3, dtype=torch.long) # Create tensor with listx = torch.tensor([5.5, 3]) # Create new Tensor based on existing Tensorx = x.new_ones(5, 3, dtype=torch.double) # Create new Tensor based on existing Tensorx = torch.randn_like(x, dtype=torch.float) # Obtain matrix size, return torch.Size supports tuple types, all tuple operationsx.size() Tensor Operation 1234567891011# Operation 1y = torch.rand(5, 3) print(x + y)# Operation 2print(torch.add(x, y)) result = torch.empty(5, 3)# Operation 3torch.add(x, y, out=result) print(result) Change Tensor shape and convert to numpy array 1234567891011121314151617# When changing size and shape of sensor, use torch.viewx = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# If only one value exists in the tensor, a numeric value can be obtained# using .item()print(x.item()) # Convert torch tensor to numpy arraya = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor Change 123456789101112131415import torchimport numpy as npx = torch.randn(1)# Run only in CUDA-enabled environments (GPU environments)if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # Create a direct tensor on a GPU y = torch.ones_like(x, device=device) # Change CPU Tensor to GPU Tensor x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/en/Pytorch-Tensor/"},{"title":"Pytorch Autograd","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. Autogard autogard package provides automatic differentiation for all operations of the Tensor. Setting the .requires_gard property to True in the torch.Tensor class tracks all operations performed on that sensor. After the calculation is complete, you can call .backward() to automatically calculate the gradient To stop Tensor from tracking records, you can call .detach() to separate from the computational history Code blocks can be wrapped with torch.no_gard(): to avoid historical tracking and memory usage Useful for evaluating models with learnable parameters, set to require_guard=True, although gradients are not required. Function Class Tensor and Function are interconnected and encode all computational processes to create non-circulating graphs Each Tensor has a .gard_fn property, which refers to the Function that generated the Tensor (The gradient_fn of the user-generated Tensor is None) The derivative calculation calls the .backward() of the Tensor Backward does not require factors if Tensor is Scally, but when it has multiple elements, it is necessary to shape the Tensor as a factor of gradient.","link":"/2021/01/25/en/Pytorch-Autograd/"},{"title":"Understanding CNN","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. CNN? Cnovolutional Neural NetworkExtracting the features of the input image through the convolutional layer andperforming classification based on them Convolutional Layer Filter (Kernel) to extract features, configured as an Activation Functionto replace the value of the filter with a nonlinear value Filter doesn’t look at the whole thing, It looks at some parts.","link":"/2021/01/27/en/Understanding-CNN/"},{"title":"Pytorch Autograd 실습","text":"This article is also available in English. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import torchx = torch.ones(2, 2, requires_grad=True) # 연산을 기록print(x)y = x + 2 # 연산 수행print(y)print(y.grad_fn) # 연산 결과이므로 grad_fn을 가짐z = y*y*3 # z = (x+2)^2*3out = z.mean()print(z, out)&quot;&quot;&quot;.requires_gard_(...)는 기존 Tensor의 requires_grad 값을 inplace 방식으로 변경.입력값 지정되지 않으면 default는 False&quot;&quot;&quot;a = torch.randn(2, 2) # 기본 requires_gard = Falsea = ((a * 3) / (a - 1))print(a.requires_grad) # Falsea.requires_grad_(True) # _가 뒤에 붙어있기 때문에 inplace 연산print(a.requires_grad) # Trueb = (a * a).sum() # True의 연산 결과이므로 grad_fn 가짐print(b.grad_fn)print(b)# 역전파(backprop)&quot;&quot;&quot;1. scalar 값의 backprop = 미분 결과에 해당 값을 넣었을 때.x = 2행 2열짜리 1이 들어있는 행렬y = x+2 == 2행 2열짜리 3이 들어있는 행렬z = y*y*3 == 2행 2열 27이 들어있는 행렬 (3*(x+2)^2)out = z의 평균&quot;&quot;&quot;out.backward() # == out.backward(torch.tensor(1.))print(out)print(x.grad)# x가 사용된 최종 연산식에서 x에대한 미분(out = 3*(x+2)^2/4 -&gt; 3*(x+2)/2)# (x = 1) (x = 1은 out.backward()에서 설정됨)&quot;&quot;&quot;2. vector의 backprop = Jacobian Matrix (m차원에서 n차원으로 가는 함수가 있다고 할 때 각각 차원에 대해 모든 편미분 값을 모아놓은 matrix) 일반적인 미분 : 1변수 함수만 다룸 편미분 : 다변수 함수에서 한 변수만 변수로, 나머지 변수는 상수로 vector의 backprop은 편미분값 matrix에 곱해주는 어떤 값&quot;&quot;&quot;x = torch.rand(3, requires_grad=True)y = x * 2 # 2xwhile y.data.norm() &lt; 1000: y = y * 2 # 2^n*xprint(y)v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v) # 2^n에 0.1, 1.0, 0.0001 곱해준 값print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): # Tensor 연산 기록 멈춤 print((x ** 2).requires_grad)print(x.requires_grad)y = x.detach() # content는 같지만 require_grad가 False인 새로운 Tensor 생성print(y.requires_grad)print(x.eq(y).all())","link":"/2021/01/26/kr/Pytorch-Autograd-Practice/"},{"title":"머신 러닝 (Machine Learning)","text":"This article is also available in English. 머신러닝 데이터를 기반으로 패턴을 학습, 결과를 예측하는 알고리즘 기법개발자가 데이터, 업무 로직의 특성을 직접 감안한 프로그램을 만들 경우 난이도, 개발복잡도가 너무 높아질 수밖에 없는 분야에 유용 머신러닝의 분류 지도학습 분류 회귀 추천 시스템 시각, 음성 인지 텍스트 분석, NLP 등 비지도학습 클러스터링 차원 축소 강화 학습 등 머신러닝의 최대 단점 데이터 의존적좋은 품질의 데이터를 갖추지 못하면 머신러닝 수행 결과도 좋을 수 없음최적의 머신러닝 알고리즘, 모델 파라미터를 구축하는 능력도 중요하지만 데이터를 효율적으로 가공, 처리, 추출하는 능력 또한 중요","link":"/2021/01/27/kr/Machine-Learning/"},{"title":"Pytorch Autograd","text":"This article is also available in English. Autogard (자동 미분) autogard 패키지는 Tensor의 모든 연산에 대한 자동 미분 제공 torch.Tensor 클래스에서 .requires_gard 속성을 True로 설정하면 해당 tensor에서 이뤄진 모든 연산들을 추적(track) 계산이 완료된 후 .backward()**를 호출하여 **변화도(gradient)를 자동으로 계산할 수 있음 Tensor가 기록 추적하는 것을 중단시키려면 .detach()**를 호출하여 연산 기록으로부터 분리할 수 있음 기록 추적 및 메모리 사용을 방지하기 위해 코드 블럭을 **with torch.no_gard(): 로 감쌀 수 있음 변화도(gradient)는 필요 없지만 requires_guard=True로 설정되어 학습 가능한 매개변수를 갖는 모델 평가 시 유용 Function 클래스 Tensor와 Function은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프를 생성 각 Tensor는 .gard_fn 속성을 갖는데, 이는 Tensor를 생성한 Function을 참고함 (사용자 생성 Tensor의 grad_fn은 None) 도함수 계산은 Tensor의 .backward() 호출 Tensor가 스칼리인 경우 backward에 인자 필요 없으나 여러 요소를 가질 때는 tensor의 모양을 gradient의 인자로 지정할 필요가 있음","link":"/2021/01/25/kr/Pytorch-Autograd/"},{"title":"Pytorch 신경망","text":"This article is also available in English. 신경망 torch.nn 패키지를 사용하여 생성 가능nn은 모델을 정의하고 미분하는데 autograd 사용nn.Module은 layer와 output을 반환하는 forward(input) 메서드 포함 신경망의 일반적인 학습 과정 학습 가능한 매개변수(또는 가중치(weight))를 갖는 신경망 정의 데이터셋 입력 반복 입력을 신경망에서 전파 손실(loss) 계산 변화도(gradient)를 신경망 매개변수들에 역전파 신경망의 가중치 갱신(새로운 가중치 = 기존 가중치 - 학습률(learning_rate) * 변화도(gradient) 신경망 정의 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() &quot;&quot;&quot; 입력 이미지 채널 1개, 출력 채널 6개, 3x3의 정사각 컨볼루션 행렬 컨볼루션 커널 정의 troch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int) : input image의 channel수. rgb면 3 out_channels(int) : convolution에 의해 생성된 channel 수 kernel_size(int or tuple) : convoling_kernel 크기. (filter) stride(int or tuple) : convolution의 stride를 얼만큼 줄 것인가. default는 1, stride는 이미지 횡단 시 커널의 스텝 사이즈 padding(int or tuple) : zero padding을 input 양쪽 인자만큼. default는 0이라서 기본적으로 설정하지 않을 경우 zero padding 적용하지 않음 self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) 아핀(affine) 연산: y = Wx + b self.fc1 = nn.Linear(16*6*6, 120) # 6*6은 이미지 차원 self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) &quot;&quot;&quot; def forward(self, x): # (2, 2) 크기 윈도우에 대해 맥스 풀링(max pooling) x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 크기가 제곱수라면 하나의 숫자만을 특정 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # 배치 차원을 제외한 모든 차원 num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 2D convolution using a kernel size of 3, stride of 1 and padding forward 함수만 정의하면, backward 함수는 autograd를 사용하여 자동으로 정의됨.forward 함수에서는 어떠한 Tensor 연산을 사용해도 됨모델의 학습 가능 매개변수는 net.parameters()에 의해 반환됨 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768params = list(net.parameters())print(len(params))print(params[0].size())# 임의의 32x32값 입력input = torch.randn(1, 1, 32, 32)out = net(input)print(out)# 모든 매개변수의 변화도 버퍼를 0으로 설정, 무작위 값으로 역전파net.zero_grad()out.backward(torch.rand(1, 10))# 손실 함수# output, target을 한 쌍의 입력으로 받아, 출력이 정답으로부터 얼마나 멀리 떨어져있는지# 추정하는 값을 계산# 간단한 손실 함수로 출력과 대상 간 평균제곱오차를 계산하는 nn.MSEloss가 있음output = net(input)target = torch.randn(10) # 비교를 위한 예시, 임의의 정답target = target.view(1, -1) # 출력과 같은 shape로 변환criterion = nn.MSELoss()loss = criterion(output, target)print(loss)&quot;&quot;&quot;input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss&quot;&quot;&quot;# 변화도가 누적된 .grad Tensorprint(loss.grad_fn)print(loss.grad_fn.next_functions[0][0])print(loss.grad_fn.next_functions[0][0].next_functions[0][0])# 역전파# 기존 변화도를 없애지 않으면 기존의 변화도에 누적됨net.zero_grad()print('conv1.bias.grad before backward') # 0으로 초기화되어있음print(net.conv1.bias.grad)loss.backward() # loss = criterion(output, target)print('conv1.bias.grad after backward')print(net.conv1.bias.grad)# 가중치 갱신# 확률적 경사하강법learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate)# torch.optim이라는 작은 패키지에 SGD, Nesterov-SGD, Adam, RMSProp등# 다양한 갱신 규칙을 구현해두었음import torch.optim as optim# Optimizer 생성optimizer = optim.SGD(net.parameters(), lr=0.01)# 학습 과정# optimizer.zero_grad()를 사용하여 수동으로 변화도 버퍼를 0으로 설정optimizer.zero_grad()output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()","link":"/2021/01/26/kr/Pytorch-Neural-Network/"},{"title":"Pytorch 텐서 생성, 연산, 변환","text":"This article is also available in English. Pytorch란? 딥러닝 프로젝트 빌드를 위해 도움을 주는 파이썬 라이브러리코어 데이터 구조인 텐서를 제공 (numpy 배열과 유사한 다차원 배열)텐서는 수학적 연산을 가속화 (GPU 사용 가능) 성능적인 이유로 대부분 C++과 CUDA로 만들어져있음. 설치방법 PyTorch 설치 링크 본인의 환경에 맞게 설정 후 Run this Command 명령어 복사 GPU 확인 1234import torchprint(torch.cuda.get_device_name(0))print(torch.cuda.is_available()) Tensor 생성 123456789101112131415# 그 시점에 할당된 메모리에 존재하던 값들이 초기값으로 나타남x = torch.empty(5, 3)# 무작위로 초기화된 행렬 (0 &lt;= x &lt; 1)x = torch.rand(5, 3) # dtype = long, 0으로 채워진 행렬x = torch.zeros(5, 3, dtype=torch.long) # list를 사용하여 직접 tensor 생성x = torch.tensor([5.5, 3]) # 기존 tensor를 바탕으로 새로운 tensor 생성x = x.new_ones(5, 3, dtype=torch.double) # 기존 tensor를 바탕으로 새로운 tensor 생성x = torch.randn_like(x, dtype=torch.float) # 행렬 크기 구하기, 반환인 torch.Size는 튜플 타입, 모든 튜플 연산 지원x.size() Tensor 연산 1234567891011# 연산 문법 1y = torch.rand(5, 3) print(x + y)# 연산 문법 2print(torch.add(x, y)) result = torch.empty(5, 3)# 연산 문법 3torch.add(x, y, out=result) print(result) Tensor shape 변경 및 numpy 배열로 변환 12345678910111213141516# tensor의 size, shape 변경 시에는 torch.view 사용x = torch.rand(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())x = torch.randn(1)print(x)# tensor에 하나의 값만 존재한다면 .item()을 사용하여 숫자 값 얻을 수 있음print(x.item()) # torch tensor를 numpy 배열로 변환a = torch.ones(5)print(a)b = a.numpy()print(b) CPU, GPU Tensor 변환 123456789101112131415import torchimport numpy as npx = torch.randn(1)# CUDA 사용 가능 환경(GPU 환경)에서만 실행if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # GPU 상 직접 tensor 생성 y = torch.ones_like(x, device=device) # CPU 텐서를 GPU 텐서로 변경 x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double))","link":"/2021/01/25/kr/Pytorch-Tensor/"},{"title":"CNN에 대한 이해","text":"This article is also available in English. CNN? Cnovolutional Neural Network컨볼루셔널 계층을 통해 입력 이미지의 특징(Feature)를 추출, 이를 기반으로 분류를 수행 Convolutional Layer 특징을 추출하는 Filter(Kernel), 필터의 값을 비선형 값으로 바꾸는 Activation Function으로 구성 Filter는 전체를 보는 것이 아닌 일부분을 집중해서 본다","link":"/2021/01/27/kr/Understanding-CNN/"},{"title":"Scikit-Learn-1 (Classification Example)","text":"이 글은 한국어로도 볼 수 있습니다. My English is not good. So if there is a grammatical error, please leave a comment. scikit-learn The most popular Python machine learning library Classifying varieties with brush flower data setPrediction of flower varieties based on petal length, width, flower support length, and width Classification One of the most representative supervised learning methods Practice Decision Tree algorithms 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pandas as pdiris = load_iris() # Load iris datasetiris_data = iris.data # numpy data consisting of only input featuresiris_label = iris.target # Prediction labelprint('iris target값:', iris_label)print('iris target명:', iris.target_names)iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) # make dataframeiris_df['label'] = iris.targetprint(iris_df.head(3))# Learn data, test data separation, feature x, label y,# 80% learning data, 20% testing datax_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=0.2, random_state=1)# Creating a DecisionTreeClassifier objectdt_clf = DecisionTreeClassifier(random_state=1)dt_clf.fit(x_train, y_train) # Model Learningpred = dt_clf.predict(x_test) # Model Predictionfrom sklearn.metrics import accuracy_score # Get Accuracyprint('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))&quot;&quot;&quot;1. Separation of data sets (learning data, test data separation)2. Model Learning3. Perform predictions4. Evaluation&quot;&quot;&quot;","link":"/2021/01/27/en/Scikit-Learn-1/"},{"title":"Scikit-Learn-1 (분류 에제)","text":"This article is also available in English. scikit-learn 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 붓꽃 데이터 세트로 붓꽃의 품종 분류하기꽃잎 길이, 너비, 꽃받침 길이, 너비를 기반으로 꽃의 품종 예측 분류(Classification) 대표적인 지도학습 방법의 하나 의사 결정 트리(Decision Tree) 알고리즘 실습 12345678910111213141516171819202122232425262728293031323334353637from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport pandas as pdiris = load_iris() # 붓꽃 데이터 세트 로드iris_data = iris.data # 입력 피처만으로 된 numpy 데이터iris_label = iris.target # 예측 labelprint('iris target값:', iris_label)print('iris target명:', iris.target_names)iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) # dataframe 생성iris_df['label'] = iris.targetprint(iris_df.head(3))# 학습 데이터, 테스트 데이터 분리, feature들을 x, label을 y로,# 80% 학습 데이터, 20% 테스트 데이터x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=0.2, random_state=1)dt_clf = DecisionTreeClassifier(random_state=1) # DecisionTreeClassifier 객체 생성dt_clf.fit(x_train, y_train) # 모델 학습pred = dt_clf.predict(x_test) # 모델 예측from sklearn.metrics import accuracy_score # 정확도 파악print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))&quot;&quot;&quot;1. 데이터 세트 분리 (학습 데이터, 테스트 데이터 분리)2. 모델 학습3. 예측 수행4. 평가&quot;&quot;&quot;","link":"/2021/01/27/kr/Scikit-Learn-1/"},{"title":"Scikit-Learn-2","text":"Estimator 이해 scikit-learn에서 분류 알고리즘을 Classifier로, 회귀 알고리즘을 Regressor로 지칭Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름모델 학습은 fit(), 학습된 모델을 사용한 예측은 predict() 메소드 제공 cross_val_score()와 같은 evaluation 함수, GridSearchCV와 같은 하이퍼파라미터 튜닝을 지원하는 클래스의 경우 Estimator를 인자로 받음 비지도학습인 차원 축소, 클러스터링, 피처 추출 등을 구현한 클래스 역시 대부분 fit(), transform() 적용비지도학습과 피처 추출에서 fit()은 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 구조 작업에 해당fit()으로 구조를 맞추면 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출 등 실제 작업은 transform()으로 수행두 기능을 결합한 fit_transform()도 함께 제공 scikit-learn 주요 패키지 sklearn.datasets datasets.make_classificiation분류를 위한 데이터 세트 생성높은 상관도, 불필요한 속성 등 노이즈 효과를 위한 데이터를 무작위로 생성 datasets.make_bolbs()클러스터링을 위한 데이터 세트 무작위 생성군집 지정 개수에 따라 여러가지 클러스터링을 위한 데이터세트 생성 사이킷런 데이터세트는 딕셔너리 형태로 되어있음 data : 피처의 데이터세트 target : 분류 시 레이블 값, 회귀 시 숫자 결과값 target_names : 개별 레이블 이름 feature_names : 피처 이름 DESCR : 데이터 세트에 대한 설명, 각 피처 설명 data, target은 넘파이 배열, target_names, feature_names는 넘파이 배열 또는 list 타입DESCR은 스트링 타입","link":"/2021/01/27/kr/Scikit-Learn-2/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"Scikit learn","slug":"Scikit-learn","link":"/tags/Scikit-learn/"},{"name":"Convolutional Neural Network","slug":"Convolutional-Neural-Network","link":"/tags/Convolutional-Neural-Network/"},{"name":"파이토치","slug":"파이토치","link":"/tags/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"머신 러닝","slug":"머신-러닝","link":"/tags/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"사이킷 런","slug":"사이킷-런","link":"/tags/%EC%82%AC%EC%9D%B4%ED%82%B7-%EB%9F%B0/"},{"name":"분류","slug":"분류","link":"/tags/%EB%B6%84%EB%A5%98/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Pytorch","slug":"Pytorch","link":"/categories/Pytorch/"}]}